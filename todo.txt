TODO:
python3 yara_rule_tester.py data.db proxies.yaml flag=fn proxy=squid
I'm basically going through looking at the flag=fn (false negatives) and trying to figure out why the rule didnt trigger for these, or vice versa for false positives
I've only done chrome, since we got barely any firefox or safari requests as it is
How can we test for referer to be different than source?
ConnectionKeepAlive marks everything as positive
Currently reading every X- header as one and the same
MITMProxy has two real differences, both in ordering (Sec_Ch_Ua_Headers_Order
    and Pragma_Cache_Control_Order) which I havent figured out yet

Evilginx : 142.93.155.177
Traefik : 159.223.56.15
MITMProxy : 147.182.196.179
Squid : 178.128.225.42
HAProxy : 170.64.227.20
Modlishka : 159.65.95.40
TinyProxy : 209.38.52.54

Precision "Of all the items labeled as positive, how many were actually positive?"
Recall "Of all the actual positives, how many did we correctly identify?"
F1 Score provides a balance of both precision and recall: F1 Score = 2 * (Precision * Recall) / (Precision + Recall) 

Best results so far:
Evilginx
    Confusion Matrix:
    True Positive (TP): 291 | 8.23% of total | 53.89% of true value
    False Positive (FP): 398 | 11.26% of total | 73.70% of true value
    True Negative (TN): 2596 | 73.46% of total | 86.71% of true value
    False Negative (FN): 249 | 7.05% of total | 8.32% of true value

    Precision: 0.42
    Recall: 0.54
    F1 Score: 0.47

Traefik
    Confusion Matrix:
    True Positive (TP): 223 | 6.31% of total | 68.83% of true value
    False Positive (FP): 1731 | 48.98% of total | 534.26% of true value
    True Negative (TN): 1479 | 41.85% of total | 46.07% of true value
    False Negative (FN): 101 | 2.86% of total | 3.15% of true value

    Precision: 0.11
    Recall: 0.69
    F1 Score: 0.20

MITMProxy
    Confusion Matrix:
    True Positive (TP): 0 | 0.00%
    False Positive (FP): 72 | 2.04%
    True Negative (TN): 3119 | 88.26%
    False Negative (FN): 343 | 9.71%

    Precision: 0.00
    Recall: 0.00
    F1 Score: 0.00

HAProxy
    Confusion Matrix:
    True Positive (TP): 225 | 6.37% of total | 59.68% of true value
    False Positive (FP): 0 | 0.00% of total
    True Negative (TN): 3157 | 89.33% of total | 100.00% of true value
    False Negative (FN): 152 | 4.30% of total

    Precision: 1.00
    Recall: 0.60
    F1 Score: 0.75

Squid
    Confusion Matrix:
    True Positive (TP): 230 | 6.51% of total | 70.34% of true value
    False Positive (FP): 0 | 0.00% of total
    True Negative (TN): 3207 | 90.75% of total | 100.00% of true value
    False Negative (FN): 97 | 2.74% of total

    Precision: 1.00
    Recall: 0.70
    F1 Score: 0.83

Modlishka
    Confusion Matrix:
    True Positive (TP): 114 | 3.23% of total | 30.89% of true value
    False Positive (FP): 0 | 0.00% of total
    True Negative (TN): 3165 | 89.56% of total | 100.00% of true value
    False Negative (FN): 255 | 7.22% of total

    Precision: 1.00
    Recall: 0.31
    F1 Score: 0.47

TinyProxy
    Confusion Matrix:
    True Positive (TP): 415 | 11.74% of total | 100.00% of true value
    False Positive (FP): 616 | 17.43% of total
    True Negative (TN): 2503 | 70.83% of total | 80.25% of true value
    False Negative (FN): 0 | 0.00% of total

    Precision: 0.40
    Recall: 1.00
    F1 Score: 0.57











